{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data and Build Vocabulary\n",
        "\n",
        "**Load Data**\n",
        "\n",
        "**Preprocess Data**\n",
        " - Tokenize\n",
        " - Lower case\n",
        " - Remove rare words\n",
        " - Subsample frequent words\n",
        "\n",
        "**Build Vocabulary**\n",
        "\n",
        "**Subsample Frequent Words**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/yifan/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35594\n",
            "58040\n"
          ]
        }
      ],
      "source": [
        "# Get sentences from Brown corpus\n",
        "sentences = brown.sents()\n",
        "# Lower case\n",
        "sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
        "\n",
        "# Count the occurences of each word\n",
        "word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "\n",
        "# Remove rare words with appearances less than \n",
        "min_count = 5\n",
        "rare_words = set([word for word,count in word_counts.items() if count < min_count])\n",
        "word_counts_rare = sum(word_counts[word] for word in rare_words)\n",
        "print(len(rare_words))\n",
        "\n",
        "# Remove rare words from sentences\n",
        "sentences = [[word if word not in rare_words else 'UNK' for word in sentence] \n",
        "             for sentence in sentences]\n",
        "\n",
        "# Count the occurences of each word\n",
        "word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "word_counts['UNK'] = word_counts_rare\n",
        "\n",
        "print(word_counts['UNK'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the frequency of each word\n",
        "total_counts = sum(word_counts.values())\n",
        "word_freqs = [count / total_counts for word,  count in word_counts.items()]\n",
        "# Calculate the probability of discarding each word\n",
        "subsample_threshold = 500\n",
        "p_discard = {}\n",
        "for word, freq in word_counts.items():\n",
        "  p_discard[word] = max(0, 1 - np.sqrt(subsample_threshold / freq)) if freq > subsample_threshold else 0\n",
        "\n",
        "# Subsample frequent words\n",
        "subsampled_sentences = []\n",
        "for sentence in sentences:\n",
        "  subsampled_sentence = [word for word in sentence if random.random() < 1- p_discard[word]]\n",
        "  subsampled_sentences.append(subsampled_sentence)\n",
        "\n",
        "sentences = subsampled_sentences\n",
        "\n",
        "# Build vocabulary after subsampling\n",
        "vocab = sorted(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Build word to index and index to word mappings\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Prepare data for training\n",
        "\n",
        "**Generate skip-gram pairs**\n",
        "\n",
        "**Negative sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_size = 5\n",
        "positive_pairs = []\n",
        "# Convert subsampled sentences to indices\n",
        "sentences_idx = [[word2idx[word] for word in sentence] \n",
        "                 for sentence in subsampled_sentences]\n",
        "for sentence in sentences_idx:\n",
        "    for i in range(len(sentence)):\n",
        "        target = sentence[i]\n",
        "        # Context window: from i-window_size to i+window_size, excluding i\n",
        "        for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "            if j != i:\n",
        "                context = sentence[j]\n",
        "                positive_pairs.append((target, context))\n",
        "\n",
        "class PositivePairsDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "positive_dataset = PositivePairsDataset(positive_pairs)\n",
        "dataloader = DataLoader(positive_dataset, batch_size=512, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([ 8338,  2470,    58,  9471,  6042,  6953,  5155, 14080,   277,   224,\n",
            "         5732, 14177,  5883,    36,  4901,  8750,  1752, 10948,  7752,  9205,\n",
            "        12828, 13776,  7084, 10110,  4608,  7358,  6026,  8614,   418, 13497,\n",
            "         1439,  5214,  8866, 10580,  7196,   309,  4361, 12902,  4935,  3633,\n",
            "         7986,  1159,  2690,   278,  3441,  8415,   655, 13339, 11994,  6056,\n",
            "         1803,  9473, 12165,  1017, 13469,  3945,  9776,  7986,  3239, 13505,\n",
            "        13000,  1409, 14078,  6450, 13464,  3518,  8750,   566,  8338,  6945,\n",
            "         8911,  8338,  1533,  5203,   692,  8109,  2506,  6946,  7079,  7655,\n",
            "         3112,  3072,  2822, 14058, 10892,    39, 11595, 13907,  6020,   829,\n",
            "        14199,  7725, 10823,  6788,  8502,  6230,  5799, 14044,  4938, 13611,\n",
            "        11921, 12806, 11501, 12193,  9503,  6315,  8134,  7222, 10297,  6684,\n",
            "         7237,  6925, 11768,  4867,  9190, 13015,  2200,  8110,  8638,  7921,\n",
            "          559, 12861,  5616,  2595,  6799,  8614,  7436,  5819,  8954,  5035,\n",
            "         7009,   719,  5914, 12798,  6020,  7309, 13212,  9757, 11427, 11849,\n",
            "         9402,  8845,   449,  2861,  8078,  8459,  9682,  7578,  6243,  4845,\n",
            "         9010,  8750,  3831,  2200, 12750, 11356,  3239,  1159, 12793,  7193,\n",
            "         3954,  9639,  1409,   727,  7394, 12798,  7076, 12574,  4082,  5572,\n",
            "         6286,  1976,  9538,   279, 12343,  3787,   565,  8982,  4134,   965,\n",
            "         3713,  3968,  6925,  1409,    36, 11769,  9557,  9919, 12950,  8889,\n",
            "         8614,    39, 12993,  8615,  1248, 13931,  7926,  5087, 12845,  4625,\n",
            "         8794,  7980, 12977,  7768,  8921, 12568,  8750, 13927,  7360,  5746,\n",
            "           36,  6756,   585,  8614,  4524,  8338,  4259,  1473,  1100,   278,\n",
            "         8076,  6316,  9483, 10696,  7801,  2355, 13696,   372, 12783,  8750,\n",
            "          826,    36, 11768,  6524,  5914, 10287,  7486,  2348, 10653,  2743,\n",
            "         5268,  5421,  5693,   488,  8981,  5380,  8750,  6549,  8821,    40,\n",
            "        12888, 13417, 10124, 11224,   411,  5961,  8554, 14056,  5085, 12798,\n",
            "        14201,  4142,  4915,  4503, 12798,  7587, 13907, 14186, 13045,  1159,\n",
            "         8911,  8613,  1156,  1351,  6524, 11920, 13778,  5693,  5035,   564,\n",
            "        13498, 11469,  6105,  3138,  4467,  6728,  4023, 12786, 12468, 14027,\n",
            "         7721,  1453,  6243,  4013, 11269, 11589,  7735, 14037,    36,  5408,\n",
            "        13927,  5705, 10166,  5556,  9724,  4224,  6132, 13887, 10754,   568,\n",
            "         2836, 13893,  4003,  5643, 11985,  8039,   442,  4623,  5722, 10807,\n",
            "         4254,  6946, 11782,  7839, 14190, 13522,  7113,  1177, 14044, 11247,\n",
            "         2894,    36, 10490,   733,    83,  6405, 12828, 10478,  7408, 13469,\n",
            "         2125,  2737,  1525, 11838,  2415,  7147,  2469,  9806,  2621,  6412,\n",
            "         3152,  6558, 11339,  2738, 13077,  2384, 11781,  9976,  6642, 10251,\n",
            "         1711,  2191,  6649,  5251,  7026,  8857,  6925,  7511, 12916,  1710,\n",
            "          277, 11824,   287,  2495,  7537,  4117,  8415,  7530, 13133,  5991,\n",
            "          277, 14138,  5122,  2426,  3848,  2644,   275,  9764,  6524,  9205,\n",
            "        12399,  1692,  3182, 14040,  6746,  2691,  3489,  5887,  8596,  3731,\n",
            "        12591,  3881,  8750,  9614,  1043, 10374,   279, 12744,    39, 12977,\n",
            "         7559,  6181,  3444,  8438,   308,  2124, 13851,  2891,  1931, 12971,\n",
            "        14111,    52, 11371,  1043,  6953,  1479, 13579,  5948,  4231,  1177,\n",
            "           36,  2215, 11882,  6845, 12165,  1234,  9069,  9939,  4363, 12786,\n",
            "         2942, 12977, 12808,   776,   737, 12370,  9614, 11229,  6845,  5855,\n",
            "         2694,  9891, 13895,  8770, 11239, 10297,  7955,  6524, 12469,  1337,\n",
            "        13302,  4247,  7485,  7752, 13926,  9873, 12793, 14078,  3439,  7590,\n",
            "         6944,  1008,  6056, 11314,  4049, 10864,  1684,  7250,   278, 13013,\n",
            "         5268,   655, 14037, 11339,  6925, 13950,   277,  3028, 10579,  1043,\n",
            "         3139, 12793, 13907,  9336,  8876,  5963,  3989,  8020, 11483, 11972,\n",
            "         6322,  8790,  7832, 12598,  1972,  1453,  4555,   268, 13774, 14199,\n",
            "         5987,  3949]), tensor([ 5364,   300, 10613, 11768, 13204, 13504, 12798, 12020,  1118,  3668,\n",
            "         8161, 12261, 11776,  2507,  8526,  2970,  6456,  3188, 12798,  5642,\n",
            "        12793,  6394, 11251, 12920,  9737, 12956, 12989,  8554, 11962,  1662,\n",
            "         4667,  8892,  3188,   156,  8954,  6324,  1409,  9462,  9150,  6837,\n",
            "        11538,    25, 11962,  7923,  5708,  5884, 11473,  5456,  2031,  8274,\n",
            "         8816,  9020,  4728,  5421,  6927, 11818,  8557, 14099,    39, 11824,\n",
            "         3072,  2137,  7182,  8268,  8373,  4524,  6550, 13285, 11856,  8229,\n",
            "         8039, 12134,  1117,  9482, 11239, 13804, 10446, 12862,   236, 11603,\n",
            "         5421,  6944,  3481, 10096,  8792,  3380, 13409, 11427,   192, 13315,\n",
            "         8794, 13291,  6446,   536, 11595,  6042,  1533,  3728, 13008, 10478,\n",
            "        12681,  2852,  6891,  7588,  6616, 12605, 10696, 12039, 14037,   278,\n",
            "         8524,  6324,  8554,  8815,  2317,  4896,  2969, 13611, 10753, 11033,\n",
            "        10578,  6394,  7923,  2595, 10281,  8330, 14044,   709,  5251,  7328,\n",
            "        12609,  2994,  9393,   684, 12874,  8884, 14075,  8716, 11727,  4955,\n",
            "         7923, 11347,  3787, 12828, 12805,  2021,  1495,   279,  2475,  8655,\n",
            "        13219, 10915,  9773, 12033,  4115, 10456, 12443, 10342,   564,  2522,\n",
            "         8010,   781,  1409, 10506, 11594,  3622,  6742,  8750,  1972,   275,\n",
            "         2063,  6782,  1453,  1547, 14109,  4680,  4482, 12204,  7337,  6499,\n",
            "         5421,  7830,  5846,  8779,  2737,  4369,  2550,  1001,  6193, 13542,\n",
            "        10053,  1972,  6116,  2950, 12253, 13973,   937, 12828, 12705,  3677,\n",
            "        12798,  1164, 10110, 12744,  5507,  3160, 13300,  6075,    36, 10277,\n",
            "          277,  9787, 12767,  5616,  1463,  7208,  5973,   684, 14044,  4003,\n",
            "         8779,  2673,  1979,  1533,  4233,  5251,  1382, 11783,  1816, 12780,\n",
            "         8260,  7537, 12861, 13698,  6013,  1204,  4224, 10969, 12735,  8810,\n",
            "         1438, 14060,    36,  7923, 12862,  7508, 13678,  8448, 12798, 13109,\n",
            "        10399,  5140,   655, 12988,    36,   418, 11595, 13389,   413,   776,\n",
            "          308, 11849, 11578,  6138, 11346,  6953,   276,  4227, 10304,   930,\n",
            "         6603,  1273,  4371,  6834, 12017, 11835,  8792, 12113,  9286,  1273,\n",
            "         8896, 11835,  8815,  5421,    25,  6060,  1550,  4790,  6944,  6138,\n",
            "         3519,  1884,  7302, 11594, 13927,    36,  6207,  3625,  8614,  7980,\n",
            "         4224,  9832,  6072,  5008,  8614,  8823,  7286, 14106,   559,    36,\n",
            "        13344,  6394,  6397,  7486,    38,   996, 13987,  3398,  2249, 10543,\n",
            "         6577, 13927, 10673,  6713, 12888,    43, 12977,   415, 10528,  6524,\n",
            "        14186,  2568, 12977,  5000,  4245, 12324,  3384, 10672, 13406,  5336,\n",
            "         8921,   860,  3824,  7858,  9392, 10239, 11797,  3553, 12606,  8499,\n",
            "        12324,  7929,  6056, 10696,  8911,  2087,  6524,  9675, 11054,  9832,\n",
            "        11727, 12885,    44, 11589,  2328,  8265,   279,  1453,  8750,  9778,\n",
            "         3577,  7346,  5019,  8884,  5993,  8719,  6925,   776, 12524, 12843,\n",
            "         6699,  5098,  4291, 12165,  6124,  5251,  4908, 14189,  7422,  5251,\n",
            "        10895,  4257,  9262,  6394, 10506, 11358,  8750, 13882,   844,  7153,\n",
            "         3359,  7341,  5332,   384, 10887,  7127, 11518,  7929, 12112,  3976,\n",
            "        12859,  9822,  1142,  4428,  5110,  1457,  7511, 13154, 13858,  6812,\n",
            "         1118,  4581,  8857,  2339, 12874,  9297,  6074,  4025,  1453, 13948,\n",
            "         1070, 10216,  6503, 13122,  8815,  3150,  7355,  9286, 10063, 11594,\n",
            "        11865,  9695,  6375,  4226,  3677,  9442, 13611, 11727, 11843, 13503,\n",
            "        11356,    39,   557,  5635,  3775,  8779,  8569,  7628, 13589, 13292,\n",
            "         6403,  2426,  3903,  6944, 10850,  8614,  9309, 11913, 12798,  1547,\n",
            "        10669, 10252, 12698,  5122,   276,   733, 13917,  2334, 12793,  2253,\n",
            "        12812,  5712,  1390, 12874,  4447,  8794, 13969,  8441,  3988,  8010,\n",
            "          737,  6039,  2140,  8794,  8479,  8619,  5140, 12608,  7909,  2249,\n",
            "         5096,  5742,  3617,   365,  4847,  5992,  1495, 13809,  8493,  2031,\n",
            "        14084,  1462])]\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "  print(batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.8449e-02, 3.5903e-05, 1.8838e-04,  ..., 3.5903e-05, 2.0399e-05,\n",
            "        1.8455e-05], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "p_w = np.array([count ** 0.75 for word, count in word_counts.items()])\n",
        "p_w = p_w / p_w.sum()\n",
        "neg_dist = torch.from_numpy(p_w).float().to(device)\n",
        "\n",
        "print(neg_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipGramNegativeSampling(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim=100):\n",
        "    super(SkipGramNegativeSampling, self).__init__()\n",
        "    self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.center_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "  \n",
        "  def forward(self, context_words, center_words):\n",
        "    v_conext = self.context_embedding(context_words)\n",
        "    v_center = self.center_embedding(center_words)\n",
        "    return torch.sum(v_center * v_conext, dim=1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "embedding_dim = 100\n",
        "model = SkipGramNegativeSampling(vocab_size, embedding_dim).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "k = 200  # Number of negative samples per positive pair\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4488.4410763941705\n",
            "Epoch 2, Loss: 272.80193928442895\n",
            "Epoch 3, Loss: 214.53017933107913\n",
            "Epoch 4, Loss: 196.2890978511423\n",
            "Epoch 5, Loss: 187.98494156077504\n",
            "Epoch 6, Loss: 183.3136646002531\n",
            "Epoch 7, Loss: 180.24367477931082\n",
            "Epoch 8, Loss: 178.00153274461627\n",
            "Epoch 9, Loss: 176.28119163960218\n",
            "Epoch 10, Loss: 174.87268908694386\n",
            "Epoch 11, Loss: 173.74177016690373\n",
            "Epoch 12, Loss: 172.8091551847756\n",
            "Epoch 13, Loss: 171.9816152434796\n",
            "Epoch 14, Loss: 171.3014553207904\n",
            "Epoch 15, Loss: 170.73111234791577\n",
            "Epoch 16, Loss: 170.1897006649524\n",
            "Epoch 17, Loss: 169.74380703270435\n",
            "Epoch 18, Loss: 169.33613689802587\n",
            "Epoch 19, Loss: 168.97610017843544\n",
            "Epoch 20, Loss: 168.6399192046374\n",
            "Epoch 21, Loss: 168.33868644945323\n",
            "Epoch 22, Loss: 168.07660743407905\n",
            "Epoch 23, Loss: 167.857492396608\n",
            "Epoch 24, Loss: 167.61907729320228\n",
            "Epoch 25, Loss: 167.41281303949654\n",
            "Epoch 26, Loss: 167.23227480240166\n",
            "Epoch 27, Loss: 167.08321898058057\n",
            "Epoch 28, Loss: 166.90361057966948\n",
            "Epoch 29, Loss: 166.77624802663922\n",
            "Epoch 30, Loss: 166.61446044407785\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        targets, contexts = batch  # Each is [batch_size]\n",
        "        targets = targets.to(device)\n",
        "        contexts = contexts.to(device)\n",
        "        B = targets.size(0)\n",
        "\n",
        "        # Generate negative samples\n",
        "        negative_contexts = torch.multinomial(neg_dist, B * k, replacement=True).view(B, k).to(device)\n",
        "\n",
        "        # Prepare full batch: positive + negative samples\n",
        "        targets_pos = targets\n",
        "        words_pos = contexts\n",
        "        labels_pos = torch.ones(B, device=device)\n",
        "\n",
        "        targets_neg = targets.unsqueeze(1).expand(-1, k).reshape(-1)  # [B * k]\n",
        "        words_neg = negative_contexts.reshape(-1)                     # [B * k]\n",
        "        labels_neg = torch.zeros(B * k, device=device)\n",
        "\n",
        "        all_targets = torch.cat([targets_pos, targets_neg], dim=0)\n",
        "        all_words = torch.cat([words_pos, words_neg], dim=0)\n",
        "        all_labels = torch.cat([labels_pos, labels_neg], dim=0)\n",
        "\n",
        "        # Forward pass\n",
        "        dot_products = model(all_targets, all_words)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(dot_products, all_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the embedding of man + king - woman = queen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 synonyms for 'morning':\n",
            "afternoon: 0.855\n",
            "evening: 0.841\n",
            "o'clock: 0.836\n",
            "day: 0.828\n",
            "next: 0.827\n"
          ]
        }
      ],
      "source": [
        "# Function to find synonyms for a given word\n",
        "def find_synonyms(word, word_to_idx, idx_to_word, embeddings, top_k=5):\n",
        "    if word not in word_to_idx:\n",
        "        print(f\"Word '{word}' not found in vocabulary\")\n",
        "        return\n",
        "    \n",
        "    # Get the word embedding\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_embedding = embeddings[word_idx]\n",
        "    \n",
        "    # Calculate cosine similarity with all words\n",
        "    similarities = torch.nn.functional.cosine_similarity(\n",
        "        word_embedding.unsqueeze(0), \n",
        "        embeddings,\n",
        "        dim=1\n",
        "    )\n",
        "    \n",
        "    # Get top k similar words (excluding the input word)\n",
        "    top_similarities, top_indices = similarities.topk(top_k + 1)\n",
        "    \n",
        "    print(f\"\\nTop {top_k} synonyms for '{word}':\")\n",
        "    for i in range(1, len(top_indices)):  # Start from 1 to skip the word itself\n",
        "        idx = top_indices[i].item()\n",
        "        similarity = top_similarities[i].item()\n",
        "        print(f\"{idx_to_word[idx]}: {similarity:.3f}\")\n",
        "\n",
        "find_synonyms('morning', word2idx, idx2word, model.center_embedding.weight.detach())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
